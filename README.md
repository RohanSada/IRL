`Env_Ackermann.py` — Ackermann Vehicle Environment

This script implements a custom reinforcement learning environment based on an Ackermann steering vehicle model, commonly used in autonomous driving research. The vehicle follows a simplified kinematic bicycle model to approximate realistic motion, including acceleration, velocity, and steering constraints. The environment is configured as a bounded two-dimensional space in which the vehicle must navigate from a starting position to a dynamically assigned goal while avoiding randomly placed circular obstacles. The obstacle configuration changes with each reset, promoting generalization in learned policies.

To sense its environment, the vehicle uses simulated LiDAR beams that cast rays in an angular arc to measure distances to obstacles. These LiDAR beams are computed efficiently using Numba’s JIT compilation to accelerate ray-casting and collision checking. The state observed by the agent consists of positional information, heading, velocity, distance and bearing to the goal, as well as the LiDAR scan data. The environment returns shaped rewards based on the vehicle’s progress toward the goal and penalizes collisions with obstacles or boundaries. The design is compatible with OpenAI Gym and Gymnasium interfaces and is suitable for training data-driven navigation agents under physical and perceptual uncertainty.


`collect_trajectories.py` — Expert Trajectory Collection

This script is designed to collect expert demonstrations using a handcrafted reactive policy in the previously defined AckermannVehicleEnv environment. The expert policy utilizes proportional control laws for both acceleration and steering to drive the vehicle toward the goal, using the Euclidean distance and angular bearing as inputs. In addition to goal-following behavior, the expert incorporates a basic obstacle avoidance mechanism: if any LiDAR beam detects an obstacle within a predefined safety margin, the controller computes a steering correction to turn away from the detected obstacle, ensuring safe navigation around hazards.

The main purpose of this script is to generate a dataset of high-quality demonstration trajectories that can be used for inverse reinforcement learning. Each trajectory consists of a sequence of (state, action) pairs recorded over multiple time steps and episodes. The collected data is saved as a serialized list of episodes using the `joblib` module. This allows for consistent reuse of expert demonstrations across different training runs. The generated dataset serves as a foundation for learning a reward function that approximates the expert’s behavior, which will later be used to train a policy using data-driven techniques.



`BNN_v2.py` — Bayesian Neural Network for Reward Learning

This script defines and trains a Bayesian Neural Network (BNN) using the Pyro probabilistic programming framework to learn a reward function from expert demonstrations. It begins by loading the expert trajectories collected previously and constructing training samples in the form of state and next-state pairs. These samples are used to compute shaped rewards that encode task-specific objectives, such as minimizing distance to the goal, avoiding obstacles, and receiving bonuses for task completion. The script applies reward shaping through a potential-based method to encourage forward progress while penalizing time and collisions.

A feature extraction process is applied to engineer a compact input space from high-dimensional state vectors. This includes metrics such as distance to the goal, the minimum, mean, and standard deviation of LiDAR readings, changes in steering angle, and a binary goal indicator. These features are standardized using scikit-learn’s `StandardScaler` to ensure better numerical stability during model training. The BNN architecture is defined with three fully connected layers, where each layer's weights and biases are treated as distributions instead of point estimates, allowing the model to quantify predictive uncertainty.

The network is trained using stochastic variational inference (SVI) with the ELBO loss function. The training process runs for a fixed number of epochs and records the ELBO at each iteration. After training, the model is evaluated using test data, and performance metrics such as mean squared error (MSE), R² score, Spearman’s ρ, and Kendall’s τ are computed to assess the quality of the learned reward predictions. Visualizations are included to compare predicted rewards against ground truth shaped rewards. Finally, the trained guide (variational approximation) and data scaler are saved to disk for use during policy training.

`train_policy.py` — Training PPO Policy with BNN-based Rewards

This script uses the Stable Baselines3 implementation of the Proximal Policy Optimization (PPO) algorithm to train a policy in the Ackermann environment, with a key difference: the reward signal is no longer derived directly from the environment. Instead, it is predicted by the previously trained Bayesian Neural Network. The script wraps the base environment with a custom Gym wrapper that intercepts the reward calculation and replaces it with the BNN’s output based on the current state and action taken. This allows the policy to learn behaviors that align with the inferred reward function obtained from expert demonstrations.

The wrapper supports three different modes of reward shaping with uncertainty: `explore`, which adds a bonus proportional to the standard deviation of the BNN prediction to encourage exploration; `risk_averse`, which subtracts this uncertainty to prefer more predictable actions; and `none`, which uses the mean reward prediction directly. Additionally, the script supports an automatic two-stage training procedure. In the first stage, the agent is trained with an exploratory reward function that encourages broad state-space coverage. In the second stage, the policy is fine-tuned using a risk-averse formulation to reinforce safe, consistent behaviors.

All training configurations, including batch size, learning rate, and number of epochs, are customizable through command-line arguments. The trained policy is saved to disk at the end of training for reuse or deployment. This training process enables the agent to develop robust navigation strategies in the Ackermann environment using a reward function that was inferred from demonstrations rather than explicitly designed, a key objective in inverse reinforcement learning.

